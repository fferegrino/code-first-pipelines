{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d68767eb-a3ca-4de2-b28c-d060ac2d8521",
   "metadata": {},
   "source": [
    "# Introduction to the Code-First Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "072d619c-e4e1-4429-a427-baa665139f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b171fd7-3650-4203-87ac-b5849c21f125",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cf_pipelines import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f189555-405a-4b86-a2e4-3566f46d5cd7",
   "metadata": {},
   "source": [
    "A pipeline has a name â€“ ideally the name of the project and a location. If you don't provide one, a temporary one will be selected fot you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f610834-1653-4713-8cd5-bef9c0532abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_pipeline = Pipeline(\"Iris pipeline\", location='outputs')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4584fc-cbd7-4a01-a96c-544460156b8c",
   "metadata": {},
   "source": [
    "## Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95209b12-e0cc-4d5c-ac70-f9db246b7489",
   "metadata": {},
   "source": [
    "### First step â€“ data ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec3d76d-94d1-4520-b0ab-63d9e05a605e",
   "metadata": {},
   "source": [
    "A pipeline is comprised of steps. To include a function as part of the pipeline you must use the `step` decorator, passing as an argument the group this particular step belongs to. For example, I have selected *\"data_ingestion\"* you can choose any name you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b84c464-40e0-4a3a-ad57-9328d18f8097",
   "metadata": {},
   "outputs": [],
   "source": [
    "@iris_pipeline.step(\"data_ingestion\")\n",
    "def get_data():\n",
    "    d = datasets.load_iris()\n",
    "    df = pd.DataFrame(d[\"data\"])\n",
    "    df.columns = d[\"feature_names\"]\n",
    "    df[\"target\"] = d[\"target\"]\n",
    "    return {\"raw_data.csv\": df}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f877efc6-ca8a-4814-807c-d53300da417b",
   "metadata": {},
   "source": [
    "A step can contain any Python code, but an important requisite is that they have to return a dictionary with at least one key-value pair:\n",
    "\n",
    " - The **key** can be any valid Python identifier (a combination of letters in lowercase or uppercase, digits and underscores), like \"raw_data\" followed by a file extension, like \".csv\"\n",
    " - The **value** that should be associated with the key\n",
    " \n",
    "These key value pairs are what we will know as a **product**.\n",
    "\n",
    "The products a function returns will be available for each subsequent step within the pipeline.\n",
    "\n",
    "#### Why do we need a file extension?\n",
    "\n",
    "The extension will determine how the product is persisted on disk. Yes! the pipelines store a copy of each product ensuring there is full reproducibility and data provenance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545a1753-fd7a-485c-a954-cf5fc1cc6d81",
   "metadata": {},
   "source": [
    "### A second step â€“ feature engineering\n",
    "\n",
    "Say I have a step that depends on a product generated by a previous step. In order for me to access it I would need to add it as a named argument for the function that needs it. For example, in the function below I want to use a dataframe from the previous step, I specify `raw_data` as a named argument, and then treat its value as I would do with any other dataframe. \n",
    "\n",
    " > ðŸš¨ Named arguments should appear after a single asterisk `*` in the function arguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c270d2c-6dc6-4668-b84a-7ed42049b207",
   "metadata": {},
   "outputs": [],
   "source": [
    "@iris_pipeline.step(\"feature_engineering\")\n",
    "def create_feature_interactions(*, raw_data):\n",
    "    ft = raw_data[\"sepal length (cm)\"] * raw_data[\"sepal width (cm)\"]\n",
    "    df = pd.DataFrame({\"sepal area (cm2)\": ft})\n",
    "    return {\"engineered_features.parquet\": df}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b8db3d-3cc2-484a-bf12-8291729ff1c6",
   "metadata": {},
   "source": [
    "Â¿See?, I am returning yet another product with a different file extension and it will be stored and available for me to consume in the next steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6959d23d-230e-4ddd-85fe-3790058457c3",
   "metadata": {},
   "source": [
    "## Third step â€“ ???\n",
    "\n",
    "In the next step I want to demonstrate two things:\n",
    "\n",
    " - A group name can be totally unrelated to machine learning (and can be amost any string), see the decorator.\n",
    " - A step can consume the products generated by any other previous steps, not necessarily only the previous one, in the function below I am using both `raw_data` and `engineered_features`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49c59246-ca8d-48f4-84f2-9406b11954cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "@iris_pipeline.step(\"another_name\")\n",
    "def join_features_and_split(*, raw_data, engineered_features):\n",
    "    training_data = raw_data.join(engineered_features)\n",
    "    \n",
    "    x = training_data.drop(\"target\", axis=\"columns\")\n",
    "    y = training_data[[\"target\"]]\n",
    "    \n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.33, random_state=42)\n",
    "\n",
    "    return {\n",
    "        \"train_x.csv\": x_train,\n",
    "        \"test_x.csv\": x_test,\n",
    "        \"train_y.csv\": y_train,\n",
    "        \"test_y.csv\": y_test,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc4c4a6-f24a-4874-965d-09e72bdf9ef8",
   "metadata": {},
   "source": [
    "## Other steps â€“ training and testing\n",
    "\n",
    "I have shown you almost all the cool bits of the `Pipelines` framework, but there is some more machine learning left to do "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2e7a392-0fb6-44d3-b04b-844201ecddfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "@iris_pipeline.step(\"training\")\n",
    "def model_training(*, train_x, train_y):\n",
    "    clf = RandomForestClassifier()\n",
    "    clf.fit(train_x, train_y[\"target\"])\n",
    "    y_pred = clf.predict(train_x)\n",
    "    return {\"model\": clf}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb86a76-40be-40a3-9dd1-45181b05a9b1",
   "metadata": {},
   "source": [
    "As you can see, the above product does not specify an extension, which means the product will be persisted using the pickle module. This may bot be the best way to store something on disk, so my recommendation is that you always try to specify an extension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ee18d6e-2320-4b0f-a017-b14b24c047dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "@iris_pipeline.step(\"model_testing\")\n",
    "def test_my_model(*, test_x, test_y, model):\n",
    "    y_pred = model.predict(test_x)\n",
    "    fig = plt.figure(figsize=(5, 5))\n",
    "    ax = fig.gca()\n",
    "    ConfusionMatrixDisplay.from_estimator(model, test_x, test_y, ax=ax)\n",
    "    return {\"matrix.png\": fig}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa13332-f55e-4379-b169-029417bc53b3",
   "metadata": {},
   "source": [
    "Finally, we can execute the pipeline by using its `run` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "726fed8e-9407-4536-bdc6-aa59817d383c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cffe4e1be8f04c709215e62f3306f48a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "iris_pipeline.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe44efc-c658-45e8-9e99-581da311a902",
   "metadata": {},
   "source": [
    "## Reviewing the outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "163c96a9-1fee-4768-966c-507e75f9efd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: tree\n"
     ]
    }
   ],
   "source": [
    "!tree outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ac26cd0-17a9-4ee9-b0cc-efda9b1df824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "head: outputs/another_name/test_x.csv: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!head outputs/another_name/test_x.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1238ae10-b20b-4a92-b14f-d354c0e1b07e",
   "metadata": {},
   "source": [
    "### Making predictions with the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c839b2a1-06d4-49fe-ab17-3c75cd1d8d50",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'outputs/training/model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/26/kknv0qdn23x8cn86y7p06j4r0000gp/T/ipykernel_26096/837933772.py\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"outputs/training/model\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mrb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mclassifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'outputs/training/model'"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"outputs/training/model\", \"rb\") as rb:\n",
    "    classifier = pickle.load(rb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8c9437-4120-4dff-8508-614385e8551a",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_instance = {\n",
    "    \"sepal length (cm)\": 6.0,\n",
    "    \"sepal width (cm)\": 3.1,\n",
    "    \"petal length (cm)\": 3.8,\n",
    "    \"petal width (cm)\": 0.5,\n",
    "    \"sepal area (cm2)\": 18.6,\n",
    "}\n",
    "instance = pd.DataFrame([new_instance])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39cb0652-4632-4ad0-a8e1-dd69047ca45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.predict(instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904eaf8e-ceb0-490f-8e40-bfc15eb40440",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
